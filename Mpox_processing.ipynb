{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd47e8ec",
   "metadata": {},
   "source": [
    "# Monkeypox ncbi data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85da45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio import SeqIO, Entrez\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "from dateutil.parser import parse\n",
    "import openpyxl\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "from urllib.error import HTTPError\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db4037",
   "metadata": {},
   "source": [
    "## GB File Downloading\n",
    "\n",
    "To download GenBank files, you need a sequences.csv file containing the NCBI MPOX list of accessions. Please make surethe sequences.csv file is saved in the same folder as this notebook.\n",
    "\n",
    "The output file (sequence.gb) will be saved in the sequence_files folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fbe6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_sequence_files(accessions, output_directory):\n",
    "    Entrez.email = 'juanfinello@gmail.com'  # Enter your email address\n",
    "\n",
    "    genbank_records = []  # List to store GenBank records\n",
    "\n",
    "    for accession in accessions:\n",
    "        try:\n",
    "            # Fetch the GenBank (full) record\n",
    "            handle = Entrez.efetch(db='nucleotide', id=accession, rettype='gb', retmode='text')\n",
    "            genbank_data = handle.read()\n",
    "            handle.close()\n",
    "            genbank_records.append(genbank_data)\n",
    "\n",
    "            print(f'Downloaded: {accession}')\n",
    "\n",
    "        except HTTPError as e:\n",
    "            print(f'Error downloading {accession}: {e}')\n",
    "\n",
    "    # Save the GenBank records to a single file\n",
    "    genbank_filename = os.path.join(output_directory, 'sequences.gb')\n",
    "    with open(genbank_filename, 'w') as f:\n",
    "        f.write('\\n'.join(genbank_records))\n",
    "\n",
    "# Read accessions from the sequences.csv file\n",
    "accessions = []\n",
    "with open('sequences.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip the header row if present\n",
    "    for row in reader:\n",
    "        accession = row[0]  # Assuming the accessions are in the first column\n",
    "        accessions.append(accession)\n",
    "\n",
    "# Define the output directory to save the downloaded files\n",
    "output_directory = 'sequence_files'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Download the sequence files for the accessions\n",
    "download_sequence_files(accessions, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fd913a",
   "metadata": {},
   "source": [
    "### Import .gb files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b6d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 - Open .gb file and create \n",
    "gb_file = \"sequences.gb\"\n",
    "genbank_df = pd.DataFrame()\n",
    "\n",
    "for gb_record in SeqIO.parse(open(gb_file, \"r\"), \"genbank\"):\n",
    "    # Attributes are: id, seq, name, description, dbxrefs, features, annotations, letter_annotations\n",
    "    try:\n",
    "        gb_lab = gb_record.annotations[\"references\"][1].journal\n",
    "    except:\n",
    "        gb_lab = gb_record.annotations[\"references\"][0].journal\n",
    "        \n",
    "    entries_dic = {\"gb_id\": gb_record.name,\n",
    "                   \"gb_description\": gb_record.description,\n",
    "                   \"gb_authors\": gb_record.annotations[\"references\"][0].authors,\n",
    "                   \"gb_lab\": gb_lab,\n",
    "                   \"gb_sequence\": str(gb_record.seq)}\n",
    "    gb_features = dict(gb_record.features[0].qualifiers)\n",
    "    \n",
    "    try:\n",
    "        gb_assembly = dict(dict(gb_record.annotations[\"structured_comment\"])[\"Assembly-Data\"])\n",
    "        gb_annotations = gb_record.annotations.pop(\"structured_comment\", \"references\")\n",
    "        gb_annotations.pop(\"Assembly-Data\")\n",
    "        all_ncbi_info = all_ncbi_info = dict(**entries_dic, **gb_features, **gb_annotations, **gb_assembly)\n",
    "        single_record_df = pd.DataFrame(all_ncbi_info)\n",
    "        genbank_df = genbank_df.append(single_record_df)\n",
    "    except:\n",
    "        gb_assembly = {\"Assembly Method\": \"\", \"Sequencing Technology\": \"\"}\n",
    "        gb_annotations = {\"Empty\": \"empty\"}\n",
    "        all_ncbi_info = all_ncbi_info = dict(**entries_dic, **gb_features, **gb_annotations, **gb_assembly)\n",
    "        single_record_df = pd.DataFrame(all_ncbi_info)\n",
    "        genbank_df = genbank_df.append(single_record_df)\n",
    "\n",
    "# Code I may use\n",
    "#    print(gb_record.annotations[\"references\"][1].journal)\n",
    "#    print(gb_record.annotations[\"references\"][0])\n",
    "#    print(dict(gb_record.features[0].qualifiers))\n",
    "#    single_record_df = pd.DataFrame(dict(gb_record.features[0].qualifiers))\n",
    "#    genbank_df = genbank_df.append(single_record_df)\n",
    "#    print(gb_record.features[0])\n",
    "#    print(dict(gb_record.features[0].qualifiers))\n",
    "#        gb_annotations = gb_record.annotations.pop(\"structured_comment\", \"references\")\n",
    "#        gb_annotations.pop(\"Assembly-Data\")\n",
    "#        all_ncbi_info = dict(entries_dic | gb_features | gb_annotations | gb_assembly)\n",
    "#        single_record_df = pd.DataFrame(all_ncbi_info)\n",
    "#        genbank_df = genbank_df.append(single_record_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5 - Error Checking and Metadata Verification\n",
    "#After processing the data, we need to check for errors. An empty column in the genbank_df DataFrame indicates whether good metadata (NaN) or bad metadata (empty) was found.\n",
    "\n",
    "genbank_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf0d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 - Write tsv file with all info (except sequence)\n",
    "genbank_df.drop(columns = [\"gb_sequence\"]).to_csv(\"original.tsv\", index = False, encoding = \"utf8\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30b68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FASTA file\n",
    "\n",
    "fasta_file = open(\"original.fasta\", \"w\")\n",
    "for key, value in pd.Series(genbank_df.gb_sequence.values, index = genbank_df.isolate).to_dict().items():\n",
    "    fasta_file.write(\">\" + key + \"\\n\" + value + \"\\n\")\n",
    "fasta_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d4be0f",
   "metadata": {},
   "source": [
    "### Metadata Import and Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44ff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import tsv, output from the script number one\n",
    "\n",
    "tsv= pd.read_csv('original.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a99c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get columns with data from tsv\n",
    "\n",
    "gb_authors =  list(tsv[\"gb_authors\"])\n",
    "gb_lab =  list(tsv[\"gb_lab\"])\n",
    "isolate =  list(tsv[\"isolate\"])\n",
    "isolate_org =  list(tsv[\"isolate\"])\n",
    "#isolation_source =  list(tsv[\"isolation_source\"])\n",
    "country =  list(tsv[\"country\"])\n",
    "collection_date =  list(tsv[\"collection_date\"])\n",
    "assembly_method =  list(tsv[\"Assembly Method\"])\n",
    "sequencing_technology =  list(tsv[\"Sequencing Technology\"])\n",
    "host = list(tsv[\"host\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04625d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling Empty Lines in Columns\n",
    "#If you encounter empty lines in the columns and the previous code gives you an error, you can use the following code snippet:\n",
    "\n",
    "#collection_date = []\n",
    "#for date in tsv[\"collection_date\"]:\n",
    "#    if isinstance(date, str) and date.strip():\n",
    "#        collection_date.append(date.strip())\n",
    "#    else:\n",
    "#        collection_date.append(\"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86fd16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Lists to Complete Columns\n",
    "# edit your submitter name and fasta file name\n",
    "\n",
    "unknown = list([\"unknown\"]* len(gb_authors))\n",
    "submitter = list([\"juanfinello\"]* len(gb_authors))\n",
    "FASTAfile = list([\"sequence_curated.fasta\"]* len(gb_authors))\n",
    "isolation_source =  list([\"unknown\"]* len(gb_authors))\n",
    "empty_col = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2245f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform the collection date format\n",
    "\n",
    "def convert_date(date):\n",
    "    try:\n",
    "        # Parse the date string using dateutil.parser\n",
    "        parsed_date = parse(date)\n",
    "\n",
    "        # Determine the format of the input date\n",
    "        num_elements = len(date.split('-'))\n",
    "        if '/' in date:\n",
    "            num_elements = len(date.split('/'))\n",
    "\n",
    "        # Format the datetime object according to the number of elements\n",
    "        if num_elements == 3:\n",
    "            formatted_date = parsed_date.strftime('%Y-%m-%d')\n",
    "        elif num_elements == 2:\n",
    "            formatted_date = parsed_date.strftime('%Y-%m')\n",
    "        elif num_elements == 1:\n",
    "            formatted_date = parsed_date.strftime('%Y')\n",
    "        else:\n",
    "            raise ValueError('Invalid date format')\n",
    "        \n",
    "        return formatted_date\n",
    "    \n",
    "    except ValueError:\n",
    "        print(f\"Invalid date format: {date}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5955dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "salida = []\n",
    "\n",
    "for date in collection_date:\n",
    "  salida.append(convert_date(date))\n",
    "\n",
    "collection_date = salida\n",
    "print(collection_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f51b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#append year to virus names \n",
    "\n",
    "# Iterate over each element in the list\n",
    "for i in range(len(collection_date)):\n",
    "    # Check if the element is a date in the Y-M-D format\n",
    "    try:\n",
    "        datetime.strptime(collection_date[i], \"%Y-%m-%d\")\n",
    "        # Extract the year from the date string\n",
    "        year = str(datetime.strptime(collection_date[i], \"%Y-%m-%d\").year)\n",
    "        # Check if the element already ends with \"/Y\" and a year\n",
    "        if isolate[i].endswith(\"/\" + year):\n",
    "            continue  # if the element ends with the year skip it\n",
    "        # Append the year to the end of the element\n",
    "        isolate[i] += \"/\" + year\n",
    "    except ValueError:\n",
    "        # If the element is not a date in the Y-M-D format, skip it\n",
    "        pass\n",
    "\n",
    "#print(isolate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77919f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace MPXV for hMpxV\n",
    "\n",
    "for i in range(len(isolate)):\n",
    "    if isolate[i].startswith(\"MPXV/\") or isolate[i].startswith(\"MPXV22/\") or isolate[i].startswith(\"MPXV23/\") or isolate[i].startswith(\"HMPXV/\"):\n",
    "        isolate[i] = \"hMpxV/\" + isolate[i][5:]\n",
    "\n",
    "print(isolate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b41677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change host if \"Homo sapiens\"\n",
    "hsapiens = 'homo sapiens'\n",
    "new_string = 'Human'\n",
    "\n",
    "for i in range(len(host)):\n",
    "    if host[i].lower() == hsapiens.lower():\n",
    "        host[i] = new_string\n",
    "\n",
    "#print(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4ad259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove \"Submitter\" and the date that appears in parentheses\n",
    "\n",
    "# Define the substring and characters to delete\n",
    "substring_to_delete = 'Submitted'\n",
    "\n",
    "# Iterate over each string in the list\n",
    "for i in range(len(gb_lab)):\n",
    "    # Find the index of the substring to delete\n",
    "    index_to_delete = gb_lab[i].find(substring_to_delete)\n",
    "    \n",
    "    # Delete the substring and parentheses if they exist\n",
    "    if index_to_delete != -1:\n",
    "        start_index = gb_lab[i].find('(')\n",
    "        end_index = gb_lab[i].find(')', start_index) + 1\n",
    "        gb_lab[i] = gb_lab[i][:start_index] + gb_lab[i][end_index:]\n",
    "        gb_lab[i] = gb_lab[i][:index_to_delete] + gb_lab[i][index_to_delete+len(substring_to_delete):]\n",
    "\n",
    "# Print the updated list\n",
    "#print(gb_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cfb790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change USA state abbreviation for full word\n",
    "\n",
    "# Define a dictionary mapping state abbreviations to full state names\n",
    "state_dict = {'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California', 'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia', 'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa', 'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland', 'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri', 'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey', 'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio', 'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina', 'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont', 'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming'}\n",
    "\n",
    "# Iterate over each string in the list\n",
    "for i in range(len(country)):\n",
    "    # Check if the string contains the word 'USA'\n",
    "    if 'USA' in country[i]:\n",
    "        # Replace each state abbreviation in the string with its full name\n",
    "        for abbr, full_name in state_dict.items():\n",
    "            country[i] = country[i].replace(abbr, full_name)\n",
    "            \n",
    "# Print the updated list\n",
    "#print(country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6b089f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create final df with template structure\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Submitter' : [submitter],\n",
    "    'FASTA filename' : [FASTAfile],\n",
    "    'Virus name' : [isolate],\n",
    "    'Passage details/history' : [empty_col],\n",
    "    'Collection date' : [collection_date],\n",
    "    'Location': [country],\n",
    "    'Additional location information': [empty_col],\n",
    "    'Host': [host],\n",
    "    'Additional Host information': [empty_col],\n",
    "    'Sampling Strategy': [empty_col],\n",
    "    'Gender': [unknown],\n",
    "    'Patient age': [unknown],\n",
    "    'Patient status': [unknown],\n",
    "    'Specimen source': [isolation_source],\n",
    "    'Outbreak': [empty_col],\n",
    "    'Last vaccinated': [empty_col],\n",
    "    'Treatment': [empty_col],\n",
    "    'Sequencing technology' : [sequencing_technology],\n",
    "    'Assembly method': [assembly_method],\n",
    "    'Depth of coverage': [empty_col],\n",
    "    'Originating lab': [gb_lab],\n",
    "    'Address': [empty_col],\n",
    "    'Sample ID given by the sample provider': [empty_col],\n",
    "    'Submitting lab': [gb_lab],\n",
    "    'Address 2': [empty_col],\n",
    "    'Sample ID given by the submitting laboratory' : [isolate_org],\n",
    "    'Authors' : [gb_authors],\n",
    "    'Comment': [empty_col],\n",
    "    'Comment Icon': [empty_col],\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfdb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2 = df.apply(pd.Series.explode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ae981d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c876aa4",
   "metadata": {},
   "source": [
    "### Export data frame to Mpox template "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd885bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Excel file into a workbook object\n",
    "book = openpyxl.load_workbook('template.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a186c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the sheet you want to copy the DataFrame to\n",
    "sheet = book['Submissions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65350433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the row to start copying the DataFrame to\n",
    "start_row = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de26026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to the sheet starting from the specified row\n",
    "for r in dataframe_to_rows(df2, index=False, header=True):\n",
    "    sheet.insert_rows(start_row)\n",
    "    for c, val in enumerate(r, 1):\n",
    "        sheet.cell(row=start_row, column=c, value=val)\n",
    "    start_row += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4becc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated Excel file\n",
    "book.save('/metadata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef0575d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39c3785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e197881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63264fbc",
   "metadata": {},
   "source": [
    "### Change Virus Names\n",
    "\n",
    "At this point, you need to go to the template1.xlsx file saved in the sequences_file folder and curate the data.\n",
    "\n",
    "Edit the virus names in the pox_virus_name column and save the previous virus names in the pox_subm_sample_id column.\n",
    "\n",
    "The following code will replace the old virus names in the FASTA file with the newly edited ones. Remember to save the changes in the metadata.xlsx file before continuing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e73ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Excel file\n",
    "excel_file = pd.ExcelFile('metadata.xlsx')\n",
    "\n",
    "# Extract the desired columns from the second sheet\n",
    "sheet_name = excel_file.sheet_names[1]  # Assuming the second sheet\n",
    "df = excel_file.parse(sheet_name)\n",
    "desired_columns = ['pox_subm_sample_id' , 'pox_virus_name']\n",
    "extracted_data = df[desired_columns]\n",
    "\n",
    "# Remove the second row (header) from each column\n",
    "extracted_data = extracted_data.iloc[1:]\n",
    "\n",
    "# Save the values in a text file\n",
    "output_file = 'vnames_to_change.txt'\n",
    "extracted_data.to_csv(output_file, sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12d45e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta= open('original.fasta', 'r') #encoding = 'utf-16')\n",
    "newnames= open('vnames_to_change.txt', 'r')\n",
    "newfasta= open('sequence_curated.fasta', 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={}\n",
    "for row in newnames:\n",
    "    n_old = row.split('\\t')[0].strip()\n",
    "    n_new = row.split('\\t')[1].strip()\n",
    "    dic[n_old]=n_new\n",
    "print (dic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e37db8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in fasta:\n",
    "    if line.startswith('>'):\n",
    "        line_name=line.split('>')[1]\n",
    "        line_name=line_name.strip()  #去除特殊符号，例如空格，\\t, \\n等\n",
    "        if line_name in dic:\n",
    "            newname= dic[line_name]\n",
    "            newname='>'+newname+'\\n'\n",
    "            newfasta.write(newname)\n",
    "        else:\n",
    "            print('Warning!!!   '+line_name+ '  not exist')\n",
    "    else:\n",
    "        newfasta.write(line)\n",
    "\n",
    "fasta.close()\n",
    "newnames.close()\n",
    "newfasta.close()\n",
    "\n",
    "##if not in order:\n",
    "##use perl\n",
    "#  type cd C:\\Strawberry\\perl\\bin in command prompt\n",
    "#then type perl Format_Fasta.pl input.fasta > output.fasta\n",
    "## linearised the sequence\n",
    "##  while read line ; do grep -A1 \"^$line\" iran_curated.fasta >> output.fasta ; done < modifying.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6a092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ede604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee646d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0bdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae629bf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a0a810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818606b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
